#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ
ÙŠØªØ¶Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ…Ø±
"""

import json
import sqlite3
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pickle
import logging
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø³Ø¬Ù„Ø§Øª
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PredictionResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªÙ†Ø¨Ø¤"""
    prediction_type: str
    confidence: float
    predicted_outcome: str
    risk_factors: List[str]
    recommendations: List[str]
    timeline: str
    probability_distribution: Dict[str, float]

@dataclass
class LearningInsight:
    """Ø±Ø¤ÙŠØ© Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù…"""
    insight_type: str
    description: str
    confidence: float
    impact_score: float
    actionable_recommendations: List[str]
    supporting_data: Dict[str, Any]

@dataclass
class ClientRiskProfile:
    """Ù…Ù„Ù Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„"""
    client_id: str
    overall_risk_score: float
    risk_categories: Dict[str, float]
    predicted_issues: List[str]
    prevention_strategies: List[str]
    monitoring_alerts: List[str]

class PredictiveIntelligenceSystem:
    """Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
    
    def __init__(self, db_path: str = "predictive_intelligence.db"):
        self.db_path = db_path
        self.models = {}
        self.scalers = {}
        self.encoders = {}
        self.feature_importance = {}
        self.learning_history = []
        self.prediction_accuracy = {}
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self._initialize_database()
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©
        self._load_models()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ
        self._initialize_ml_models()
        
        logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±")

    def _initialize_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS predictions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                client_id TEXT,
                prediction_type TEXT,
                predicted_outcome TEXT,
                confidence REAL,
                actual_outcome TEXT,
                accuracy REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                validated_at TIMESTAMP,
                features TEXT
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learning_insights (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                insight_type TEXT,
                description TEXT,
                confidence REAL,
                impact_score REAL,
                supporting_data TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                applied BOOLEAN DEFAULT FALSE
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS risk_profiles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                client_id TEXT UNIQUE,
                overall_risk_score REAL,
                risk_categories TEXT,
                predicted_issues TEXT,
                prevention_strategies TEXT,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS training_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                client_id TEXT,
                features TEXT,
                outcome TEXT,
                outcome_type TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS model_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                model_name TEXT,
                accuracy REAL,
                precision_score REAL,
                recall REAL,
                f1_score REAL,
                training_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                data_size INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()

    def _initialize_ml_models(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ"""
        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø±
        self.models['risk_prediction'] = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        
        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¯ÙØ¹
        self.models['payment_prediction'] = GradientBoostingRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=42
        )
        
        # Ù†Ù…ÙˆØ°Ø¬ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
        self.models['client_clustering'] = KMeans(
            n_clusters=5,
            random_state=42
        )
        
        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª
        self.models['violation_prediction'] = RandomForestClassifier(
            n_estimators=80,
            max_depth=8,
            random_state=42
        )
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.scalers['standard'] = StandardScaler()
        self.encoders['label'] = LabelEncoder()

    def _load_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª"""
        try:
            with open('models/predictive_models.pkl', 'rb') as f:
                saved_models = pickle.load(f)
                self.models.update(saved_models)
            logger.info("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ø¨Ù†Ø¬Ø§Ø­")
        except FileNotFoundError:
            logger.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø©ØŒ Ø³ÙŠØªÙ… Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ø¬Ø¯ÙŠØ¯Ø©")

    def _save_models(self):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨Ø©"""
        import os
        os.makedirs('models', exist_ok=True)
        
        with open('models/predictive_models.pkl', 'wb') as f:
            pickle.dump(self.models, f)
        
        with open('models/scalers.pkl', 'wb') as f:
            pickle.dump(self.scalers, f)
        
        with open('models/encoders.pkl', 'wb') as f:
            pickle.dump(self.encoders, f)

    def extract_client_features(self, client_data: Dict[str, Any]) -> np.ndarray:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„"""
        features = []
        
        # Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        contracts = client_data.get('contracts', [])
        payments = client_data.get('payments', [])
        violations = client_data.get('violations', [])
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‚ÙˆØ¯
        features.append(len(contracts))
        
        # Ù…ØªÙˆØ³Ø· Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ù‚ÙˆØ¯
        if contracts:
            avg_contract_value = np.mean([c.get('amount', 0) for c in contracts])
            features.append(avg_contract_value)
        else:
            features.append(0)
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©
        overdue_payments = len([p for p in payments if p.get('payment_status') == 'overdue'])
        features.append(overdue_payments)
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©
        if payments:
            overdue_ratio = overdue_payments / len(payments)
            features.append(overdue_ratio)
        else:
            features.append(0)
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª
        features.append(len(violations))
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª Ø§Ù„Ù†Ø´Ø·Ø©
        active_violations = len([v for v in violations if v.get('status') == 'active'])
        features.append(active_violations)
        
        # Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù…Ø³ØªØ­Ù‚
        if payments:
            avg_amount = np.mean([p.get('amount', 0) for p in payments])
            features.append(avg_amount)
        else:
            features.append(0)
        
        # Ø¹Ù…Ø± Ø£Ù‚Ø¯Ù… Ø¹Ù‚Ø¯ (Ø¨Ø§Ù„Ø£ÙŠØ§Ù…)
        if contracts:
            oldest_contract = min([
                (datetime.now() - datetime.strptime(c.get('start_date', '2024-01-01'), '%Y-%m-%d')).days
                for c in contracts
            ])
            features.append(oldest_contract)
        else:
            features.append(0)
        
        # Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ù†Ø´Ø·Ø©
        active_contracts = len([c for c in contracts if c.get('status') == 'active'])
        features.append(active_contracts)
        
        # Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù…Ø³ØªØ­Ù‚
        total_due = sum([p.get('amount', 0) for p in payments if p.get('payment_status') in ['pending', 'overdue']])
        features.append(total_due)
        
        return np.array(features).reshape(1, -1)

    def predict_client_risk(self, client_data: Dict[str, Any]) -> PredictionResult:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„"""
        try:
            features = self.extract_client_features(client_data)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ·Ø¨ÙŠØ¹
            if hasattr(self.scalers['standard'], 'mean_'):
                features_scaled = self.scalers['standard'].transform(features)
            else:
                features_scaled = features
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø±
            if hasattr(self.models['risk_prediction'], 'predict_proba'):
                risk_proba = self.models['risk_prediction'].predict_proba(features_scaled)[0]
                risk_classes = self.models['risk_prediction'].classes_
                
                # Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
                probability_distribution = dict(zip(risk_classes, risk_proba))
                
                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹
                max_risk_idx = np.argmax(risk_proba)
                predicted_risk = risk_classes[max_risk_idx]
                confidence = risk_proba[max_risk_idx]
            else:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø© Ù„Ù„ØªÙ†Ø¨Ø¤
                features_flat = features.flatten()
                
                risk_score = 0
                risk_factors = []
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±
                if features_flat[2] > 2:  # Ù…Ø¯ÙÙˆØ¹Ø§Øª Ù…ØªØ£Ø®Ø±Ø©
                    risk_score += 0.3
                    risk_factors.append("Ù…Ø¯ÙÙˆØ¹Ø§Øª Ù…ØªØ£Ø®Ø±Ø© Ù…ØªØ¹Ø¯Ø¯Ø©")
                
                if features_flat[3] > 0.5:  # Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©
                    risk_score += 0.25
                    risk_factors.append("Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©")
                
                if features_flat[4] > 3:  # Ù…Ø®Ø§Ù„ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
                    risk_score += 0.2
                    risk_factors.append("Ù…Ø®Ø§Ù„ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©")
                
                if features_flat[5] > 1:  # Ù…Ø®Ø§Ù„ÙØ§Øª Ù†Ø´Ø·Ø©
                    risk_score += 0.15
                    risk_factors.append("Ù…Ø®Ø§Ù„ÙØ§Øª Ù†Ø´Ø·Ø©")
                
                if features_flat[9] > 10000:  # Ù…Ø¨Ù„Øº Ù…Ø³ØªØ­Ù‚ ÙƒØ¨ÙŠØ±
                    risk_score += 0.1
                    risk_factors.append("Ù…Ø¨Ù„Øº Ù…Ø³ØªØ­Ù‚ ÙƒØ¨ÙŠØ±")
                
                # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±
                if risk_score >= 0.7:
                    predicted_risk = "Ø¹Ø§Ù„ÙŠ"
                    confidence = min(risk_score, 0.95)
                elif risk_score >= 0.4:
                    predicted_risk = "Ù…ØªÙˆØ³Ø·"
                    confidence = min(risk_score + 0.1, 0.85)
                else:
                    predicted_risk = "Ù…Ù†Ø®ÙØ¶"
                    confidence = max(1 - risk_score, 0.6)
                
                probability_distribution = {
                    "Ù…Ù†Ø®ÙØ¶": max(0, 1 - risk_score - 0.3),
                    "Ù…ØªÙˆØ³Ø·": min(0.6, risk_score + 0.2),
                    "Ø¹Ø§Ù„ÙŠ": min(risk_score, 0.8)
                }
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = self._generate_risk_recommendations(predicted_risk, client_data)
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ
            timeline = self._estimate_risk_timeline(predicted_risk, client_data)
            
            return PredictionResult(
                prediction_type="client_risk",
                confidence=confidence,
                predicted_outcome=predicted_risk,
                risk_factors=risk_factors if 'risk_factors' in locals() else [],
                recommendations=recommendations,
                timeline=timeline,
                probability_distribution=probability_distribution
            )
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„: {e}")
            return PredictionResult(
                prediction_type="client_risk",
                confidence=0.5,
                predicted_outcome="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                risk_factors=["Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"],
                recommendations=["ÙŠØ±Ø¬Ù‰ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ¯ÙˆÙŠØ§Ù‹"],
                timeline="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                probability_distribution={}
            )

    def predict_payment_behavior(self, client_data: Dict[str, Any]) -> PredictionResult:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¯ÙØ¹"""
        try:
            features = self.extract_client_features(client_data)
            
            # ØªØ­Ù„ÙŠÙ„ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª
            payments = client_data.get('payments', [])
            
            if not payments:
                return PredictionResult(
                    prediction_type="payment_behavior",
                    confidence=0.6,
                    predicted_outcome="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                    risk_factors=["Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¯ÙØ¹ ÙƒØ§ÙÙŠØ©"],
                    recommendations=["Ù…Ø±Ø§Ù‚Ø¨Ø© Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¯ÙØ¹ Ø§Ù„Ø£ÙˆÙ„ÙŠ"],
                    timeline="30 ÙŠÙˆÙ…",
                    probability_distribution={}
                )
            
            # Ø­Ø³Ø§Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¯ÙØ¹
            overdue_count = len([p for p in payments if p.get('payment_status') == 'overdue'])
            total_payments = len(payments)
            overdue_ratio = overdue_count / total_payments if total_payments > 0 else 0
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠØ©
            payment_delays = []
            for payment in payments:
                if payment.get('payment_status') == 'overdue':
                    due_date = datetime.strptime(payment.get('due_date', '2024-01-01'), '%Y-%m-%d')
                    delay_days = (datetime.now() - due_date).days
                    payment_delays.append(delay_days)
            
            avg_delay = np.mean(payment_delays) if payment_delays else 0
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¯ÙØ¹
            if overdue_ratio >= 0.6:
                predicted_behavior = "Ù…ØªØ£Ø®Ø± Ù…Ø²Ù…Ù†"
                confidence = 0.85
                risk_factors = ["Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©", f"Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ£Ø®ÙŠØ±: {avg_delay:.0f} ÙŠÙˆÙ…"]
            elif overdue_ratio >= 0.3:
                predicted_behavior = "Ù…ØªØ£Ø®Ø± Ø£Ø­ÙŠØ§Ù†Ø§Ù‹"
                confidence = 0.75
                risk_factors = ["Ø¨Ø¹Ø¶ Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©", "Ù†Ù…Ø· ØºÙŠØ± Ù…Ù†ØªØ¸Ù…"]
            elif overdue_ratio >= 0.1:
                predicted_behavior = "Ù…Ù†ØªØ¸Ù… Ù†Ø³Ø¨ÙŠØ§Ù‹"
                confidence = 0.7
                risk_factors = ["ØªØ£Ø®ÙŠØ±Ø§Øª Ù‚Ù„ÙŠÙ„Ø©"]
            else:
                predicted_behavior = "Ù…Ù†ØªØ¸Ù…"
                confidence = 0.8
                risk_factors = []
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = self._generate_payment_recommendations(predicted_behavior, overdue_ratio)
            
            # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ
            if predicted_behavior == "Ù…ØªØ£Ø®Ø± Ù…Ø²Ù…Ù†":
                timeline = "Ø®Ù„Ø§Ù„ 7-14 ÙŠÙˆÙ…"
            elif predicted_behavior == "Ù…ØªØ£Ø®Ø± Ø£Ø­ÙŠØ§Ù†Ø§Ù‹":
                timeline = "Ø®Ù„Ø§Ù„ 15-30 ÙŠÙˆÙ…"
            else:
                timeline = "Ø®Ù„Ø§Ù„ 30-60 ÙŠÙˆÙ…"
            
            probability_distribution = {
                "Ù…Ù†ØªØ¸Ù…": max(0, 1 - overdue_ratio * 2),
                "Ù…ØªØ£Ø®Ø± Ø£Ø­ÙŠØ§Ù†Ø§Ù‹": min(0.6, overdue_ratio * 1.5),
                "Ù…ØªØ£Ø®Ø± Ù…Ø²Ù…Ù†": min(overdue_ratio * 2, 0.9)
            }
            
            return PredictionResult(
                prediction_type="payment_behavior",
                confidence=confidence,
                predicted_outcome=predicted_behavior,
                risk_factors=risk_factors,
                recommendations=recommendations,
                timeline=timeline,
                probability_distribution=probability_distribution
            )
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¯ÙØ¹: {e}")
            return PredictionResult(
                prediction_type="payment_behavior",
                confidence=0.5,
                predicted_outcome="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                risk_factors=["Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"],
                recommendations=["Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ¯ÙˆÙŠØ§Ù‹"],
                timeline="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                probability_distribution={}
            )

    def predict_legal_issues(self, client_data: Dict[str, Any]) -> PredictionResult:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©"""
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
            contracts = client_data.get('contracts', [])
            payments = client_data.get('payments', [])
            violations = client_data.get('violations', [])
            
            legal_risk_score = 0
            potential_issues = []
            risk_factors = []
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚ÙˆØ¯
            expired_contracts = [c for c in contracts if c.get('status') == 'expired']
            if expired_contracts:
                legal_risk_score += 0.2
                potential_issues.append("Ø¹Ù‚ÙˆØ¯ Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©")
                risk_factors.append(f"{len(expired_contracts)} Ø¹Ù‚Ø¯ Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ£Ø®Ø±Ø©
            overdue_payments = [p for p in payments if p.get('payment_status') == 'overdue']
            if len(overdue_payments) >= 3:
                legal_risk_score += 0.3
                potential_issues.append("Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØ­ØµÙŠÙ„")
                risk_factors.append(f"{len(overdue_payments)} Ù…Ø¯ÙÙˆØ¹Ø§Øª Ù…ØªØ£Ø®Ø±Ø©")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª
            active_violations = [v for v in violations if v.get('status') == 'active']
            if active_violations:
                legal_risk_score += 0.25
                potential_issues.append("Ù…Ø®Ø§Ù„ÙØ§Øª Ù†Ø´Ø·Ø©")
                risk_factors.append(f"{len(active_violations)} Ù…Ø®Ø§Ù„ÙØ© Ù†Ø´Ø·Ø©")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ù„Øº Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
            total_overdue = sum([p.get('amount', 0) for p in overdue_payments])
            if total_overdue > 50000:
                legal_risk_score += 0.15
                potential_issues.append("Ù…Ø¨Ø§Ù„Øº ÙƒØ¨ÙŠØ±Ø© Ù…Ø³ØªØ­Ù‚Ø©")
                risk_factors.append(f"Ù…Ø¨Ù„Øº Ù…Ø³ØªØ­Ù‚: {total_overdue:,.0f}")
            
            # ØªØ­Ù„ÙŠÙ„ Ù…Ø¯Ø© Ø§Ù„ØªØ£Ø®ÙŠØ±
            long_overdue = []
            for payment in overdue_payments:
                due_date = datetime.strptime(payment.get('due_date', '2024-01-01'), '%Y-%m-%d')
                days_overdue = (datetime.now() - due_date).days
                if days_overdue > 90:
                    long_overdue.append(payment)
            
            if long_overdue:
                legal_risk_score += 0.1
                potential_issues.append("ØªØ£Ø®ÙŠØ±Ø§Øª Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰")
                risk_factors.append(f"{len(long_overdue)} Ù…Ø¯ÙÙˆØ¹Ø§Øª Ù…ØªØ£Ø®Ø±Ø© Ø£ÙƒØ«Ø± Ù…Ù† 90 ÙŠÙˆÙ…")
            
            # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
            if legal_risk_score >= 0.7:
                predicted_outcome = "Ù…Ø®Ø§Ø·Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ù„ÙŠØ©"
                confidence = 0.9
                timeline = "Ø®Ù„Ø§Ù„ 1-2 Ø£Ø³Ø¨ÙˆØ¹"
            elif legal_risk_score >= 0.4:
                predicted_outcome = "Ù…Ø®Ø§Ø·Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…ØªÙˆØ³Ø·Ø©"
                confidence = 0.8
                timeline = "Ø®Ù„Ø§Ù„ 1-2 Ø´Ù‡Ø±"
            elif legal_risk_score >= 0.2:
                predicted_outcome = "Ù…Ø®Ø§Ø·Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ù…Ù†Ø®ÙØ¶Ø©"
                confidence = 0.7
                timeline = "Ø®Ù„Ø§Ù„ 3-6 Ø£Ø´Ù‡Ø±"
            else:
                predicted_outcome = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø®Ø§Ø·Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ø¶Ø­Ø©"
                confidence = 0.8
                timeline = "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª
            recommendations = self._generate_legal_recommendations(potential_issues, legal_risk_score)
            
            probability_distribution = {
                "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø®Ø§Ø·Ø±": max(0, 1 - legal_risk_score),
                "Ù…Ø®Ø§Ø·Ø± Ù…Ù†Ø®ÙØ¶Ø©": min(0.4, legal_risk_score * 0.8),
                "Ù…Ø®Ø§Ø·Ø± Ù…ØªÙˆØ³Ø·Ø©": min(0.5, legal_risk_score * 1.2),
                "Ù…Ø®Ø§Ø·Ø± Ø¹Ø§Ù„ÙŠØ©": min(legal_risk_score, 0.9)
            }
            
            return PredictionResult(
                prediction_type="legal_issues",
                confidence=confidence,
                predicted_outcome=predicted_outcome,
                risk_factors=risk_factors,
                recommendations=recommendations,
                timeline=timeline,
                probability_distribution=probability_distribution
            )
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {e}")
            return PredictionResult(
                prediction_type="legal_issues",
                confidence=0.5,
                predicted_outcome="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                risk_factors=["Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"],
                recommendations=["Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙŠØ¯ÙˆÙŠØ©"],
                timeline="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                probability_distribution={}
            )

    def generate_learning_insights(self) -> List[LearningInsight]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¤Ù‰ Ù…Ù† Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
        insights = []
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ØªØ­Ù„ÙŠÙ„ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            cursor.execute('''
                SELECT prediction_type, AVG(accuracy), COUNT(*) 
                FROM predictions 
                WHERE accuracy IS NOT NULL 
                GROUP BY prediction_type
            ''')
            
            accuracy_data = cursor.fetchall()
            
            for pred_type, avg_accuracy, count in accuracy_data:
                if avg_accuracy < 0.7 and count >= 10:
                    insights.append(LearningInsight(
                        insight_type="model_performance",
                        description=f"Ø¯Ù‚Ø© Ù†Ù…ÙˆØ°Ø¬ {pred_type} Ù…Ù†Ø®ÙØ¶Ø© ({avg_accuracy:.2%})",
                        confidence=0.9,
                        impact_score=0.8,
                        actionable_recommendations=[
                            "Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©",
                            "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©",
                            "ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
                        ],
                        supporting_data={
                            "current_accuracy": avg_accuracy,
                            "sample_size": count,
                            "target_accuracy": 0.8
                        }
                    ))
            
            # ØªØ­Ù„ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
            cursor.execute('''
                SELECT features, outcome 
                FROM training_data 
                WHERE created_at >= date('now', '-30 days')
            ''')
            
            recent_data = cursor.fetchall()
            
            if len(recent_data) >= 20:
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø§Ø´Ø¦Ø©
                outcomes = [json.loads(row[1]) if row[1] else {} for row in recent_data]
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø©
                pattern_analysis = self._analyze_emerging_patterns(outcomes)
                
                if pattern_analysis:
                    insights.append(LearningInsight(
                        insight_type="emerging_patterns",
                        description="ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡",
                        confidence=0.75,
                        impact_score=0.7,
                        actionable_recommendations=[
                            "ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡",
                            "ØªØ·ÙˆÙŠØ± Ù‚ÙˆØ§Ø¹Ø¯ Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„ØªÙ†Ø¨Ø¤",
                            "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©"
                        ],
                        supporting_data=pattern_analysis
                    ))
            
            # ØªØ­Ù„ÙŠÙ„ ÙØ¹Ø§Ù„ÙŠØ© Ø§Ù„ØªÙˆØµÙŠØ§Øª
            cursor.execute('''
                SELECT description, AVG(impact_score), COUNT(*) 
                FROM learning_insights 
                WHERE applied = TRUE 
                GROUP BY description
            ''')
            
            recommendation_effectiveness = cursor.fetchall()
            
            for desc, avg_impact, count in recommendation_effectiveness:
                if avg_impact > 0.8 and count >= 5:
                    insights.append(LearningInsight(
                        insight_type="successful_strategy",
                        description=f"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù†Ø§Ø¬Ø­Ø©: {desc}",
                        confidence=0.85,
                        impact_score=avg_impact,
                        actionable_recommendations=[
                            "ØªÙˆØ³ÙŠØ¹ ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©",
                            "ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ±ÙŠÙ‚ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬",
                            "ØªÙˆØ«ÙŠÙ‚ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª"
                        ],
                        supporting_data={
                            "success_rate": avg_impact,
                            "applications": count
                        }
                    ))
            
            conn.close()
            
            # Ø­ÙØ¸ Ø§Ù„Ø±Ø¤Ù‰ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self._save_learning_insights(insights)
            
            logger.info(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(insights)} Ø±Ø¤ÙŠØ© ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù…: {e}")
        
        return insights

    def create_client_risk_profile(self, client_id: str, client_data: Dict[str, Any]) -> ClientRiskProfile:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù…Ø®Ø§Ø·Ø± Ø´Ø§Ù…Ù„ Ù„Ù„Ø¹Ù…ÙŠÙ„"""
        try:
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
            risk_prediction = self.predict_client_risk(client_data)
            payment_prediction = self.predict_payment_behavior(client_data)
            legal_prediction = self.predict_legal_issues(client_data)
            
            # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
            risk_scores = {
                'financial': self._calculate_financial_risk(client_data),
                'legal': self._calculate_legal_risk(client_data),
                'operational': self._calculate_operational_risk(client_data),
                'compliance': self._calculate_compliance_risk(client_data)
            }
            
            overall_risk_score = np.mean(list(risk_scores.values()))
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
            predicted_issues = []
            if risk_prediction.predicted_outcome in ['Ø¹Ø§Ù„ÙŠ', 'Ù…ØªÙˆØ³Ø·']:
                predicted_issues.extend(risk_prediction.risk_factors)
            
            if payment_prediction.predicted_outcome in ['Ù…ØªØ£Ø®Ø± Ù…Ø²Ù…Ù†', 'Ù…ØªØ£Ø®Ø± Ø£Ø­ÙŠØ§Ù†Ø§Ù‹']:
                predicted_issues.append("Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ø¯ÙØ¹")
            
            if legal_prediction.predicted_outcome != "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø®Ø§Ø·Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ø¶Ø­Ø©":
                predicted_issues.extend(legal_prediction.risk_factors)
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ÙˆÙ‚Ø§ÙŠØ©
            prevention_strategies = self._generate_prevention_strategies(risk_scores, predicted_issues)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
            monitoring_alerts = self._generate_monitoring_alerts(risk_scores, client_data)
            
            risk_profile = ClientRiskProfile(
                client_id=client_id,
                overall_risk_score=overall_risk_score,
                risk_categories=risk_scores,
                predicted_issues=predicted_issues,
                prevention_strategies=prevention_strategies,
                monitoring_alerts=monitoring_alerts
            )
            
            # Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„Ù…Ø®Ø§Ø·Ø±
            self._save_risk_profile(risk_profile)
            
            return risk_profile
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„: {e}")
            return ClientRiskProfile(
                client_id=client_id,
                overall_risk_score=0.5,
                risk_categories={},
                predicted_issues=["Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"],
                prevention_strategies=["Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ© Ù…Ø·Ù„ÙˆØ¨Ø©"],
                monitoring_alerts=["ÙØ­Øµ Ø¯ÙˆØ±ÙŠ"]
            )

    def continuous_learning_update(self, prediction_id: str, actual_outcome: str, feedback_score: float):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ¹Ù„ÙŠØ©"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©
            cursor.execute('''
                UPDATE predictions 
                SET actual_outcome = ?, accuracy = ?, validated_at = CURRENT_TIMESTAMP
                WHERE id = ?
            ''', (actual_outcome, feedback_score, prediction_id))
            
            # Ø¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ø¬Ø¯ÙŠØ¯Ø©
            cursor.execute('''
                SELECT features, prediction_type FROM predictions WHERE id = ?
            ''', (prediction_id,))
            
            result = cursor.fetchone()
            if result:
                features, pred_type = result
                
                cursor.execute('''
                    INSERT INTO training_data (client_id, features, outcome, outcome_type)
                    VALUES (?, ?, ?, ?)
                ''', ('unknown', features, actual_outcome, pred_type))
            
            conn.commit()
            conn.close()
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            self._check_retrain_models()
            
            logger.info(f"ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù„Ù„ØªÙ†Ø¨Ø¤ {prediction_id}")
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±: {e}")

    def _generate_risk_recommendations(self, risk_level: str, client_data: Dict[str, Any]) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±"""
        recommendations = []
        
        if risk_level == "Ø¹Ø§Ù„ÙŠ":
            recommendations.extend([
                "Ù…Ø±Ø§Ø¬Ø¹Ø© ÙÙˆØ±ÙŠØ© Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„",
                "ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªØ­ØµÙŠÙ„ ØµØ§Ø±Ù…Ø©",
                "Ø§Ù„Ù†Ø¸Ø± ÙÙŠ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¹Ù‚Ø¯",
                "Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¹Ø§Ø¬Ù„Ø©"
            ])
        elif risk_level == "Ù…ØªÙˆØ³Ø·":
            recommendations.extend([
                "Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¯ÙØ¹",
                "Ø¥Ø±Ø³Ø§Ù„ ØªØ°ÙƒÙŠØ±Ø§Øª Ù…Ù†ØªØ¸Ù…Ø©",
                "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø±ÙˆØ· Ø§Ù„Ø¹Ù‚Ø¯",
                "ØªÙ‚ÙŠÙŠÙ… Ø¶Ù…Ø§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"
            ])
        else:
            recommendations.extend([
                "Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¯ÙˆØ±ÙŠØ©",
                "Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„Ø¬ÙŠØ¯",
                "ØªÙ‚Ø¯ÙŠÙ… Ø®Ø¯Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"
            ])
        
        return recommendations

    def _generate_payment_recommendations(self, behavior: str, overdue_ratio: float) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¯ÙØ¹"""
        recommendations = []
        
        if behavior == "Ù…ØªØ£Ø®Ø± Ù…Ø²Ù…Ù†":
            recommendations.extend([
                "ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ø¯ÙØ¹ ØªÙ„Ù‚Ø§Ø¦ÙŠ",
                "ØªÙ‚Ù„ÙŠÙ„ ÙØªØ±Ø§Øª Ø§Ù„Ø¯ÙØ¹",
                "Ø·Ù„Ø¨ Ø¶Ù…Ø§Ù†Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©",
                "Ø§Ù„Ù†Ø¸Ø± ÙÙŠ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„ØªØ¹Ø§Ù…Ù„"
            ])
        elif behavior == "Ù…ØªØ£Ø®Ø± Ø£Ø­ÙŠØ§Ù†Ø§Ù‹":
            recommendations.extend([
                "Ø¥Ø±Ø³Ø§Ù„ ØªØ°ÙƒÙŠØ±Ø§Øª Ù…Ø¨ÙƒØ±Ø©",
                "ØªÙ‚Ø¯ÙŠÙ… Ø­ÙˆØ§ÙØ² Ù„Ù„Ø¯ÙØ¹ Ø§Ù„Ù…Ø¨ÙƒØ±",
                "Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ø§Ø³ØªØ­Ù‚Ø§Ù‚"
            ])
        else:
            recommendations.extend([
                "Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ù…ØªØ§Ø²Ø©",
                "ØªÙ‚Ø¯ÙŠÙ… Ø®ØµÙˆÙ…Ø§Øª Ù„Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…Ù†ØªØ¸Ù…ÙŠÙ†"
            ])
        
        return recommendations

    def _generate_legal_recommendations(self, issues: List[str], risk_score: float) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
        recommendations = []
        
        if risk_score >= 0.7:
            recommendations.extend([
                "Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙˆØ±ÙŠØ©",
                "Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©",
                "ØªÙ‚ÙŠÙŠÙ… Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªÙ‚Ø§Ø¶ÙŠ",
                "Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¯Ù„Ø©"
            ])
        elif risk_score >= 0.4:
            recommendations.extend([
                "Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆÙ‚Ø§Ø¦ÙŠØ©",
                "ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹Ù‚ÙˆØ¯ ÙˆØ§Ù„Ø´Ø±ÙˆØ·",
                "ØªÙˆØ«ÙŠÙ‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª"
            ])
        else:
            recommendations.extend([
                "Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¯ÙˆØ±ÙŠØ© Ù„Ù„Ø§Ù…ØªØ«Ø§Ù„",
                "ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³ÙŠØ§Ø³Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©"
            ])
        
        return recommendations

    def _estimate_risk_timeline(self, risk_level: str, client_data: Dict[str, Any]) -> str:
        """ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ù…Ø®Ø§Ø·Ø±"""
        if risk_level == "Ø¹Ø§Ù„ÙŠ":
            return "Ø®Ù„Ø§Ù„ 1-2 Ø£Ø³Ø¨ÙˆØ¹"
        elif risk_level == "Ù…ØªÙˆØ³Ø·":
            return "Ø®Ù„Ø§Ù„ 1-3 Ø£Ø´Ù‡Ø±"
        else:
            return "Ø®Ù„Ø§Ù„ 6-12 Ø´Ù‡Ø±"

    def _calculate_financial_risk(self, client_data: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø§Ù„ÙŠØ©"""
        payments = client_data.get('payments', [])
        if not payments:
            return 0.5
        
        overdue_count = len([p for p in payments if p.get('payment_status') == 'overdue'])
        return min(overdue_count / len(payments) * 2, 1.0)

    def _calculate_legal_risk(self, client_data: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
        violations = client_data.get('violations', [])
        contracts = client_data.get('contracts', [])
        
        risk = 0
        if violations:
            active_violations = len([v for v in violations if v.get('status') == 'active'])
            risk += min(active_violations * 0.2, 0.6)
        
        if contracts:
            expired_contracts = len([c for c in contracts if c.get('status') == 'expired'])
            risk += min(expired_contracts * 0.15, 0.4)
        
        return min(risk, 1.0)

    def _calculate_operational_risk(self, client_data: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„ØªØ´ØºÙŠÙ„ÙŠØ©"""
        contracts = client_data.get('contracts', [])
        if not contracts:
            return 0.3
        
        # ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¹Ù‚ÙˆØ¯
        complex_contracts = len([c for c in contracts if len(c.get('terms', '')) > 1000])
        return min(complex_contracts / len(contracts), 0.8)

    def _calculate_compliance_risk(self, client_data: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„"""
        violations = client_data.get('violations', [])
        if not violations:
            return 0.1
        
        # ØªØ­Ù„ÙŠÙ„ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª
        serious_violations = len([v for v in violations if 'serious' in v.get('description', '').lower()])
        return min(serious_violations * 0.3, 0.9)

    def _generate_prevention_strategies(self, risk_scores: Dict[str, float], issues: List[str]) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ÙˆÙ‚Ø§ÙŠØ©"""
        strategies = []
        
        if risk_scores.get('financial', 0) > 0.6:
            strategies.append("ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ø§Ù„ÙŠØ© ØµØ§Ø±Ù…")
        
        if risk_scores.get('legal', 0) > 0.5:
            strategies.append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¯ÙˆØ±ÙŠØ©")
        
        if risk_scores.get('operational', 0) > 0.4:
            strategies.append("ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª")
        
        return strategies

    def _generate_monitoring_alerts(self, risk_scores: Dict[str, float], client_data: Dict[str, Any]) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
        alerts = []
        
        if risk_scores.get('financial', 0) > 0.5:
            alerts.append("Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø¯ÙÙˆØ¹Ø§Øª Ø£Ø³Ø¨ÙˆØ¹ÙŠØ§Ù‹")
        
        if risk_scores.get('legal', 0) > 0.4:
            alerts.append("ÙØ­Øµ Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„ Ø´Ù‡Ø±ÙŠØ§Ù‹")
        
        alerts.append("Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© ÙƒÙ„ 3 Ø£Ø´Ù‡Ø±")
        
        return alerts

    def _analyze_emerging_patterns(self, outcomes: List[Dict]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø§Ø´Ø¦Ø©"""
        if not outcomes:
            return {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø£Ù†Ù…Ø§Ø·
        pattern_counts = Counter()
        
        for outcome in outcomes:
            if isinstance(outcome, dict):
                for key, value in outcome.items():
                    pattern_counts[f"{key}:{value}"] += 1
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹
        common_patterns = pattern_counts.most_common(5)
        
        return {
            "total_samples": len(outcomes),
            "common_patterns": common_patterns,
            "pattern_diversity": len(pattern_counts)
        }

    def _save_learning_insights(self, insights: List[LearningInsight]):
        """Ø­ÙØ¸ Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù… ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for insight in insights:
            cursor.execute('''
                INSERT INTO learning_insights 
                (insight_type, description, confidence, impact_score, supporting_data)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                insight.insight_type,
                insight.description,
                insight.confidence,
                insight.impact_score,
                json.dumps(insight.supporting_data, ensure_ascii=False)
            ))
        
        conn.commit()
        conn.close()

    def _save_risk_profile(self, profile: ClientRiskProfile):
        """Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO risk_profiles 
            (client_id, overall_risk_score, risk_categories, predicted_issues, prevention_strategies)
            VALUES (?, ?, ?, ?, ?)
        ''', (
            profile.client_id,
            profile.overall_risk_score,
            json.dumps(profile.risk_categories, ensure_ascii=False),
            json.dumps(profile.predicted_issues, ensure_ascii=False),
            json.dumps(profile.prevention_strategies, ensure_ascii=False)
        ))
        
        conn.commit()
        conn.close()

    def _check_retrain_models(self):
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ÙØ­Øµ Ø¹Ø¯Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        cursor.execute('''
            SELECT COUNT(*) FROM training_data 
            WHERE created_at >= date('now', '-7 days')
        ''')
        
        new_data_count = cursor.fetchone()[0]
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©
        if new_data_count >= 50:
            logger.info("Ø¨Ø¯Ø¡ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
            self._retrain_models()
        
        conn.close()

    def _retrain_models(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            cursor.execute('''
                SELECT features, outcome, outcome_type FROM training_data
                WHERE created_at >= date('now', '-30 days')
            ''')
            
            training_data = cursor.fetchall()
            
            if len(training_data) < 20:
                logger.info("Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ ØºÙŠØ± ÙƒØ§ÙÙŠØ©")
                return
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨
            # Ù‡Ø°Ø§ Ù…Ø«Ø§Ù„ Ù…Ø¨Ø³Ø· - ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù†Ø­ØªØ§Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹
            
            logger.info(f"ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù€ {len(training_data)} Ø¹ÙŠÙ†Ø©")
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø¯Ø«Ø©
            self._save_models()
            
            conn.close()
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {e}")

    def get_system_performance_metrics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            cursor.execute('''
                SELECT prediction_type, AVG(accuracy), COUNT(*) 
                FROM predictions 
                WHERE accuracy IS NOT NULL 
                GROUP BY prediction_type
            ''')
            
            accuracy_by_type = {row[0]: {"accuracy": row[1], "count": row[2]} 
                              for row in cursor.fetchall()}
            
            # Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©
            cursor.execute('''
                SELECT COUNT(*) FROM learning_insights WHERE applied = TRUE
            ''')
            applied_insights = cursor.fetchone()[0]
            
            # Ù…ØªÙˆØ³Ø· Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ£Ø«ÙŠØ±
            cursor.execute('''
                SELECT AVG(impact_score) FROM learning_insights
            ''')
            avg_impact = cursor.fetchone()[0] or 0
            
            conn.close()
            
            return {
                "prediction_accuracy": accuracy_by_type,
                "applied_insights": applied_insights,
                "average_impact_score": avg_impact,
                "total_predictions": sum([data["count"] for data in accuracy_by_type.values()]),
                "system_health": "Ù…Ù…ØªØ§Ø²" if avg_impact > 0.7 else "Ø¬ÙŠØ¯" if avg_impact > 0.5 else "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†"
            }
            
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡: {e}")
            return {}

def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±"""
    print("=== Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± ===")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…
    system = PredictiveIntelligenceSystem()
    
    # Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù…ÙŠÙ„ ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    test_client_data = {
        "personal_info": {
            "id": "client_test_001",
            "name": "Ø£Ø­Ù…Ø¯ Ù…Ø­Ù…Ø¯ Ø§Ù„ÙƒÙˆÙŠØªÙŠ",
            "id_number": "123456789",
            "address": "Ø§Ù„ÙƒÙˆÙŠØªØŒ Ø­ÙˆÙ„ÙŠ",
            "phone": "+965-1234-5678",
            "email": "ahmed@example.com"
        },
        "contracts": [
            {
                "id": "contract_001",
                "start_date": "2024-01-01",
                "end_date": "2024-12-31",
                "status": "active",
                "amount": 15000,
                "terms": "Ø¹Ù‚Ø¯ ØªØ£Ø¬ÙŠØ± Ø³ÙŠØ§Ø±Ø© Ù„Ù…Ø¯Ø© Ø³Ù†Ø©"
            },
            {
                "id": "contract_002",
                "start_date": "2023-06-01",
                "end_date": "2024-05-31",
                "status": "expired",
                "amount": 12000,
                "terms": "Ø¹Ù‚Ø¯ ØªØ£Ø¬ÙŠØ± Ø³ÙŠØ§Ø±Ø© Ù…Ù†ØªÙ‡ÙŠ"
            }
        ],
        "payments": [
            {
                "id": "payment_001",
                "amount": 1500,
                "payment_status": "overdue",
                "due_date": "2024-07-01"
            },
            {
                "id": "payment_002",
                "amount": 1500,
                "payment_status": "overdue",
                "due_date": "2024-06-01"
            },
            {
                "id": "payment_003",
                "amount": 1500,
                "payment_status": "paid",
                "due_date": "2024-05-01"
            }
        ],
        "violations": [
            {
                "id": "violation_001",
                "description": "ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©",
                "status": "active",
                "date": "2024-07-15"
            },
            {
                "id": "violation_002",
                "description": "ÙˆÙ‚ÙˆÙ Ø®Ø§Ø·Ø¦",
                "status": "resolved",
                "date": "2024-06-10"
            }
        ]
    }
    
    print("\nğŸ”® Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„:")
    risk_prediction = system.predict_client_risk(test_client_data)
    print(f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {risk_prediction.predicted_outcome}")
    print(f"Ø§Ù„Ø«Ù‚Ø©: {risk_prediction.confidence:.2%}")
    print(f"Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±: {', '.join(risk_prediction.risk_factors)}")
    print(f"Ø§Ù„ØªÙˆØµÙŠØ§Øª: {', '.join(risk_prediction.recommendations[:2])}")
    
    print("\nğŸ’³ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¯ÙØ¹:")
    payment_prediction = system.predict_payment_behavior(test_client_data)
    print(f"Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {payment_prediction.predicted_outcome}")
    print(f"Ø§Ù„Ø«Ù‚Ø©: {payment_prediction.confidence:.2%}")
    print(f"Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ: {payment_prediction.timeline}")
    
    print("\nâš–ï¸ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:")
    legal_prediction = system.predict_legal_issues(test_client_data)
    print(f"Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©: {legal_prediction.predicted_outcome}")
    print(f"Ø§Ù„Ø«Ù‚Ø©: {legal_prediction.confidence:.2%}")
    print(f"Ø§Ù„ØªÙˆØµÙŠØ§Øª: {', '.join(legal_prediction.recommendations[:2])}")
    
    print("\nğŸ“Š Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù…Ø®Ø§Ø·Ø± Ø´Ø§Ù…Ù„:")
    risk_profile = system.create_client_risk_profile("client_test_001", test_client_data)
    print(f"Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {risk_profile.overall_risk_score:.2f}")
    print(f"Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {len(risk_profile.predicted_issues)}")
    print(f"Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ÙˆÙ‚Ø§ÙŠØ©: {len(risk_profile.prevention_strategies)}")
    
    print("\nğŸ§  Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¤Ù‰ Ø§Ù„ØªØ¹Ù„Ù…:")
    learning_insights = system.generate_learning_insights()
    print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©: {len(learning_insights)}")
    
    for insight in learning_insights[:2]:
        print(f"- {insight.description} (Ø«Ù‚Ø©: {insight.confidence:.2%})")
    
    print("\nğŸ“ˆ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…:")
    performance_metrics = system.get_system_performance_metrics()
    print(f"Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…: {performance_metrics.get('system_health', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}")
    print(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª: {performance_metrics.get('total_predictions', 0)}")
    print(f"Ù…ØªÙˆØ³Ø· Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ£Ø«ÙŠØ±: {performance_metrics.get('average_impact_score', 0):.2f}")
    
    print("\nâœ… ØªÙ… Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø¨Ù†Ø¬Ø§Ø­!")
    print("ğŸ¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù…Ø®Ø§Ø·Ø± ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø±")

if __name__ == "__main__":
    main()

