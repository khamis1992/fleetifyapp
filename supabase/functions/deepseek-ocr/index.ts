/**
 * DeepSeek VL2 OCR Edge Function
 *
 * Uses DeepSeek VL2 Vision API for document text extraction.
 * Unlike OpenAI, DeepSeek does NOT refuse to process personal documents!
 * 
 * Benefits:
 * - 90% cheaper than OpenAI
 * - No content policy refusals for documents
 * - High accuracy for Arabic text
 * - OpenAI-compatible API format
 */

import "https://deno.land/x/xhr@0.1.0/mod.ts";
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

interface OCRRequest {
  imageDataUrls: string[];
  language?: 'ar' | 'en' | 'both';
  detail?: 'low' | 'high' | 'auto';
  provider?: 'deepseek' | 'qwen'; // Allow switching between providers
}

interface OCRResponse {
  success: boolean;
  text?: string;
  confidence: number;
  pagesProcessed: number;
  method: 'deepseek' | 'qwen';
  error?: string;
}

// DeepSeek VL2 API endpoint (OpenAI-compatible)
const DEEPSEEK_API_URL = 'https://api.deepseek.com/v1/chat/completions';

// Qwen VL API endpoint (via Dashscope)
const QWEN_API_URL = 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions';

serve(async (req) => {
  // Handle CORS preflight
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { imageDataUrls, language = 'ar', detail = 'high', provider = 'deepseek' }: OCRRequest = await req.json();

    console.log('ğŸ“„ DeepSeek/Qwen OCR Request received:', {
      pageCount: imageDataUrls?.length || 0,
      language,
      detail,
      provider,
    });

    // Validate input
    if (!imageDataUrls || !Array.isArray(imageDataUrls) || imageDataUrls.length === 0) {
      throw new Error('No images provided for OCR processing');
    }

    if (imageDataUrls.length > 10) {
      throw new Error('Too many pages - maximum 10 pages per request');
    }

    // Get API key based on provider
    let apiKey: string | undefined;
    let apiUrl: string;
    let modelName: string;

    if (provider === 'qwen') {
      apiKey = Deno.env.get('QWEN_API_KEY') || Deno.env.get('DASHSCOPE_API_KEY');
      apiUrl = QWEN_API_URL;
      modelName = 'qwen-vl-max'; // or qwen-vl-plus
    } else {
      // Default to DeepSeek
      apiKey = Deno.env.get('DEEPSEEK_API_KEY');
      apiUrl = DEEPSEEK_API_URL;
      modelName = 'deepseek-vl2'; // DeepSeek Vision model
    }

    if (!apiKey) {
      console.error(`${provider} API key not configured`);
      throw new Error(`${provider} API key not configured. Please set ${provider === 'qwen' ? 'QWEN_API_KEY' : 'DEEPSEEK_API_KEY'} in Supabase environment.`);
    }

    console.log(`ğŸ” Processing ${imageDataUrls.length} page(s) with ${provider}...`);

    // Extract text from all pages
    const pageTexts: string[] = [];

    for (let i = 0; i < imageDataUrls.length; i++) {
      const imageUrl = imageDataUrls[i];
      console.log(`ğŸ“– Processing page ${i + 1}/${imageDataUrls.length}...`);

      try {
        const pageText = await extractTextFromImage(
          imageUrl,
          apiKey,
          apiUrl,
          modelName,
          language,
          detail,
          provider
        );

        if (pageText && pageText.trim().length > 0) {
          pageTexts.push(pageText.trim());
          console.log(`âœ… Page ${i + 1}: ${pageText.trim().length} characters extracted`);
        } else {
          console.warn(`âš ï¸  Page ${i + 1}: No text extracted`);
        }
      } catch (error) {
        console.error(`âŒ Error processing page ${i + 1}:`, error);
        pageTexts.push(`[Error processing page ${i + 1}: ${error}]`);
      }
    }

    // Combine all page texts
    const fullText = pageTexts.join('\n\n--- ØµÙØ­Ø© Ø¬Ø¯ÙŠØ¯Ø© ---\n\n');

    if (!fullText || fullText.trim().length === 0) {
      throw new Error('No text could be extracted from any pages');
    }

    // Calculate confidence score
    const confidence = calculateConfidence(fullText, language);

    console.log(`âœ… OCR complete: ${fullText.length} characters extracted`);
    console.log(`ğŸ“Š Confidence score: ${Math.round(confidence * 100)}%`);

    const response: OCRResponse = {
      success: true,
      text: fullText,
      confidence,
      pagesProcessed: imageDataUrls.length,
      method: provider as 'deepseek' | 'qwen',
    };

    return new Response(JSON.stringify(response), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error: unknown) {
    console.error('âŒ OCR Error:', error);

    const errorMessage = error instanceof Error ? error.message : 'Unknown error occurred';

    const response: OCRResponse = {
      success: false,
      confidence: 0,
      pagesProcessed: 0,
      method: 'deepseek',
      error: errorMessage,
    };

    return new Response(JSON.stringify(response), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});

/**
 * Extract text from a single image using DeepSeek/Qwen Vision API
 */
async function extractTextFromImage(
  imageUrl: string,
  apiKey: string,
  apiUrl: string,
  modelName: string,
  language: string,
  detail: string,
  provider: string
): Promise<string> {
  // Build the prompt for document extraction
  const systemPrompt = buildSystemPrompt(language);
  const userPrompt = language === 'ar'
    ? 'Ø§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„ Ø§Ù„Ù†Øµ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©. Ù‡Ø°Ø§ Ù…Ø³ØªÙ†Ø¯ ØªØ¬Ø§Ø±ÙŠ/Ø¹Ù‚Ø¯. Ø§Ø³ØªØ®Ø±Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„ØªÙˆØ§Ø±ÙŠØ®.'
    : 'Extract all text from this image with high accuracy. This is a business document/contract. Extract all details including names, numbers, and dates.';

  console.log(`ğŸ“¤ Calling ${provider} Vision API...`);

  // Build message content based on provider
  const imageContent = buildImageContent(imageUrl, detail, provider);

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: modelName,
      messages: [
        {
          role: 'system',
          content: systemPrompt
        },
        {
          role: 'user',
          content: [
            { type: 'text', text: userPrompt },
            ...imageContent
          ]
        }
      ],
      max_tokens: 4096,
      temperature: 0.1,
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error(`${provider} API error:`, response.status, errorText);
    throw new Error(`${provider} Vision API error: ${response.status} - ${errorText}`);
  }

  const result = await response.json();

  if (!result.choices || result.choices.length === 0) {
    throw new Error(`No response from ${provider} Vision API`);
  }

  const extractedText = result.choices[0].message.content || '';

  console.log(`ğŸ“¥ ${provider} response: ${extractedText.length} characters`);

  return extractedText;
}

/**
 * Build image content based on provider format
 */
function buildImageContent(imageUrl: string, detail: string, provider: string): any[] {
  // Both DeepSeek and Qwen use OpenAI-compatible format
  return [{
    type: 'image_url',
    image_url: {
      url: imageUrl,
      detail: detail
    }
  }];
}

/**
 * Build system prompt for document extraction
 */
function buildSystemPrompt(language: string): string {
  if (language === 'ar') {
    return `
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ù…Ù…Ø³ÙˆØ­Ø© Ø¶ÙˆØ¦ÙŠØ§Ù‹.

Ù…Ù‡Ù…ØªÙƒ:
1. Ø§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¯Ù‚Ø© 100%
2. Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£ØµÙ„ÙŠ (ÙÙ‚Ø±Ø§ØªØŒ Ø¬Ø¯Ø§ÙˆÙ„ØŒ Ø£Ø¹Ù…Ø¯Ø©)
3. Ø§Ø³ØªØ®Ø±Ø¬ Ø¬Ù…ÙŠØ¹:
   - Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ ÙƒØ§Ù…Ù„Ø© (Ø¹Ø±Ø¨ÙŠØ© ÙˆØ¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
   - Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‡ÙˆÙŠØ© (QID) ÙˆØ§Ù„Ù‡ÙˆØ§ØªÙ
   - Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø¨Ø¬Ù…ÙŠØ¹ ØµÙŠØºÙ‡Ø§
   - Ø§Ù„Ù…Ø¨Ø§Ù„Øº Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø¹Ù…Ù„Ø§Øª
   - Ø£Ø±Ù‚Ø§Ù… Ù„ÙˆØ­Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø§Øª
   - ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø±ÙƒØ¨Ø§Øª (Ø§Ù„Ù…Ø§Ø±ÙƒØ©ØŒ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ØŒ Ø§Ù„Ø³Ù†Ø©)
4. Ù„Ø§ ØªØ¶Ù Ø£ÙŠ ØªÙØ³ÙŠØ±Ø§Øª - ÙÙ‚Ø· Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù…
5. Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ØºÙŠØ± ÙˆØ§Ø¶Ø­ØŒ Ø¶Ø¹ [ØºÙŠØ± ÙˆØ§Ø¶Ø­]

Ø£Ø®Ø±Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù… ÙÙ‚Ø·.
`;
  }

  return `
You are an expert in extracting text from scanned documents and contracts.

Your task:
1. Extract ALL text from the image with 100% accuracy
2. Preserve original formatting (paragraphs, tables, columns)
3. Extract all:
   - Full names (Arabic and English)
   - ID numbers (QID) and phone numbers
   - Dates in all formats
   - Financial amounts and currencies
   - Vehicle plate numbers
   - Vehicle details (make, model, year)
4. Do not add any interpretations - raw text only
5. If text is unclear, mark with [unclear]

Output raw text only.
`;
}

/**
 * Calculate confidence score based on extracted text quality
 */
function calculateConfidence(text: string, language: string): number {
  if (!text || text.length < 10) return 0;

  let confidence = 0.4; // Higher base for DeepSeek

  // Arabic text presence
  if (/[\u0600-\u06FF]/.test(text)) {
    confidence += 0.2;
    if (text.length > 200) confidence += 0.1;
  }

  // Numbers (QID, phone)
  if (/\d{8,}/.test(text)) confidence += 0.1;

  // Dates
  if (/\d{2}[\/\-\.]\d{2}[\/\-\.]\d{4}/.test(text)) confidence += 0.1;

  // Length bonus
  if (text.length > 100) confidence += 0.05;
  if (text.length > 500) confidence += 0.05;

  // Document markers
  if (/Ø¹Ù‚Ø¯|contract|Ø§ØªÙØ§Ù‚ÙŠØ©|agreement|Ø¥ÙŠØ¬Ø§Ø±|rental/i.test(text)) {
    confidence += 0.1;
  }

  return Math.min(confidence, 1.0);
}
